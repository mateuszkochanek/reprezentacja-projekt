{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6309fb25-aa68-433b-a236-bc3ab117573e",
   "metadata": {},
   "source": [
    "# Wielomodalny Autoenkoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0522a11-e513-4d3f-9bb5-1e30c11ee37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Type\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "from abc import abstractmethod\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0c0b1-b7a7-4203-92ca-553410a38eae",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62fe3c41-53d1-411f-bffb-ef0d13460a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        super().__init__()\n",
    "        self._df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._df.shape[0]\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        index: int,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        return dict(self._df.iloc[index])\n",
    "\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        test_path: str,\n",
    "        batch_size: int = 64,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        train_df = pd.read_pickle(train_path)\n",
    "        test_df = pd.read_pickle(test_path)\n",
    "\n",
    "        self.df = {\n",
    "            \"train\": train_df,\n",
    "            \"test\": test_df,\n",
    "            \"all\": pd.concat([train_df, test_df]),\n",
    "        }\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(\"train\")\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(\"test\")\n",
    "\n",
    "    def all_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(\"all\")\n",
    "\n",
    "    def _dataloader(self, split: str) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            MyDataset(self.df[split]),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=split == \"train\",\n",
    "            num_workers=int(os.environ.get(\"NUM_WORKERS\", 0)),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c6d9e1-0614-4ddd-a93c-3a75933a9ecd",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e571b7c7-fc84-4a33-a757-c42291743ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        modality_names: List[str],\n",
    "        in_dims: Dict[str, int],\n",
    "        hidden_dims: List[int],\n",
    "        out_dim: int,\n",
    "        last_activation: Type[nn.Module],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.modality_names = modality_names\n",
    "        \n",
    "        self.modality_to_encoder_map = nn.ModuleDict()\n",
    "        for modality_name in modality_names:\n",
    "            self.modality_to_encoder_map[modality_name] = nn.Sequential(\n",
    "                nn.Linear(in_dims[modality_name], hidden_dims[0]),\n",
    "                nn.ReLU(inplace=True),\n",
    "                *[\n",
    "                    layer\n",
    "                    for idx in range(len(hidden_dims) - 1)\n",
    "                    for layer in (nn.Linear(hidden_dims[idx], hidden_dims[idx + 1]), nn.ReLU(inplace=True))\n",
    "                ],\n",
    "                nn.Linear(hidden_dims[-1], out_dim),\n",
    "                last_activation(),\n",
    "            )\n",
    "            \n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> List[torch.Tensor]:\n",
    "        return [\n",
    "            self.modality_to_encoder_map[modality_name](x[modality_name])\n",
    "            for modality_name in self.modality_names\n",
    "        ]\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_hparams(hparams):\n",
    "        return MultimodalEncoder(\n",
    "            modality_names=hparams[\"modality_names\"],\n",
    "            in_dims=hparams[\"data_dims\"],\n",
    "            hidden_dims=hparams[\"hidden_dims\"],\n",
    "            out_dim=hparams[\"emb_dim\"],\n",
    "            last_activation=nn.Tanh,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7aee2414-67f0-4568-a541-96fe780b85da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgFusion(nn.Module):\n",
    "    \n",
    "    def forward(self, h: List[torch.Tensor]) -> torch.Tensor:\n",
    "        return sum(h) / len(h)\n",
    "    \n",
    "    \n",
    "class MLPFusion(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        modality_dim: int,\n",
    "        num_modalities: int,\n",
    "        hidden_dims: List[int],\n",
    "        out_dim: int,\n",
    "        last_activation: Type[nn.Module],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(modality_dim * num_modalities, hidden_dims[0]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            *[\n",
    "                layer\n",
    "                for idx in range(len(hidden_dims) - 1)\n",
    "                for layer in (nn.Linear(hidden_dims[idx], hidden_dims[idx + 1]), nn.ReLU(inplace=True))\n",
    "            ],\n",
    "            nn.Linear(hidden_dims[-1], out_dim),\n",
    "            last_activation(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, h: List[torch.Tensor]) -> torch.Tensor:\n",
    "        mlp_input = torch.cat(h, dim=1)\n",
    "        return self.mlp(mlp_input)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef1aa40b-9ec3-41a5-8514-02dd79fdc205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDecoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        modality_names: List[str],\n",
    "        in_dim: int,\n",
    "        hidden_dims: List[int],\n",
    "        out_dims: Dict[str, int],\n",
    "        last_activation: Type[nn.Module],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.modality_names = modality_names\n",
    "        \n",
    "        self.modality_to_encoder_map = nn.ModuleDict()\n",
    "        for modality_name in modality_names:\n",
    "            self.modality_to_encoder_map[modality_name] = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden_dims[0]),\n",
    "                nn.ReLU(inplace=True),\n",
    "                *[\n",
    "                    layer\n",
    "                    for idx in range(len(hidden_dims) - 1)\n",
    "                    for layer in (nn.Linear(hidden_dims[idx], hidden_dims[idx + 1]), nn.ReLU(inplace=True))\n",
    "                ],\n",
    "                nn.Linear(hidden_dims[-1], out_dims[modality_name]),\n",
    "                last_activation(),\n",
    "            )\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        return {\n",
    "            modality_name: self.modality_to_encoder_map[modality_name](z)\n",
    "            for modality_name in self.modality_names\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ad31046-226b-455f-9481-3f53762c9722",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAE(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, hparams, encoder: nn.Module, decoder: nn.Module):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(hparams)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def training_step(self, batch, batch_idx: int):\n",
    "        return {\"loss\": self._common_step(batch)}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = self._summarize_outputs(outputs)\n",
    "\n",
    "        self.log(\"step\", self.trainer.current_epoch)\n",
    "        self.log(\"train/loss\", avg_loss, on_epoch=True, on_step=False)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx: int):\n",
    "        return {\"loss\": self._common_step(batch)}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = self._summarize_outputs(outputs)\n",
    "\n",
    "        self.log(\"step\", self.trainer.current_epoch)\n",
    "        self.log(\"val/loss\", avg_loss, on_epoch=True, on_step=False)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _common_step(self, batch) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def _summarize_outputs(outputs):\n",
    "        losses = [out[\"loss\"] for out in outputs]\n",
    "\n",
    "        avg_loss = np.mean([loss.cpu() for loss in losses])\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            params=self.parameters(),\n",
    "            lr=self.hparams[\"lr\"],\n",
    "            weight_decay=self.hparams[\"weight_decay\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2a478d0-d103-4cd4-8606-7324ce7806e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "\n",
    "class MultimodalAE(BaseAE):\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        encoder_cls = hparams[\"encoder_cls\"]\n",
    "        \n",
    "        super().__init__(\n",
    "            hparams=hparams,\n",
    "            encoder=encoder_cls.from_hparams(hparams),\n",
    "            decoder=MultimodalDecoder(\n",
    "                modality_names=hparams[\"modality_names\"],\n",
    "                in_dim=hparams[\"emb_dim\"],\n",
    "                hidden_dims=hparams[\"hidden_dims\"][::-1],\n",
    "                out_dims=hparams[\"data_dims\"],\n",
    "                last_activation=nn.Identity,\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        if hparams[\"fusion\"] == \"Avg\":\n",
    "            self.fusion = AvgFusion()\n",
    "        elif hparams[\"fusion\"] == \"MLP\":\n",
    "            self.fusion = MLPFusion(\n",
    "                modality_dim=hparams[\"emb_dim\"],\n",
    "                num_modalities=len(hparams[\"modality_names\"]),\n",
    "                hidden_dims=[hparams[\"emb_dim\"], hparams[\"emb_dim\"]],\n",
    "                out_dim=hparams[\"emb_dim\"],\n",
    "                last_activation=nn.Tanh,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion module: \\\"{hparams['fusion']}\\\"\")\n",
    "\n",
    "    def forward(self, batch) -> torch.Tensor:\n",
    "        encoded = self.encoder(batch)\n",
    "        return self.fusion(encoded)\n",
    "\n",
    "    def _common_step(self, batch) -> torch.Tensor:\n",
    "        z = self.forward(batch)\n",
    "        x_rec = self.decoder(z)\n",
    "        mse = 0\n",
    "        for modality_name in x_rec:\n",
    "            mse += mse_loss(batch[modality_name], x_rec[modality_name])\n",
    "        return mse / len(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd0fc58-8fbd-4052-84ec-cbc9688b7cc9",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8a41e42-d37e-4923-a6d4-7daf2cd9cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model_cls: Type[pl.LightningModule],\n",
    "    hparams,\n",
    "    datamodule: DataModule,\n",
    "    accelerator=\"gpu\",\n",
    "):\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    model = model_cls(hparams)\n",
    "\n",
    "    model_chkpt = ModelCheckpoint(\n",
    "        dirpath=f\"./data/checkpoints/{hparams['name']}/\",\n",
    "        filename=\"model\",\n",
    "        monitor=\"val/loss\",\n",
    "        mode=\"min\",\n",
    "        verbose=True,\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        logger=TensorBoardLogger(\n",
    "            save_dir=\"./data/logs\",\n",
    "            name=hparams[\"name\"],\n",
    "            default_hp_metric=False,\n",
    "        ),\n",
    "        callbacks=[model_chkpt],\n",
    "        num_sanity_val_steps=0,\n",
    "        log_every_n_steps=1,\n",
    "        max_epochs=hparams[\"num_epochs\"],\n",
    "        accelerator=accelerator,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model=model, datamodule=datamodule)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embeddings(\n",
    "    model_cls: Type[pl.LightningModule],\n",
    "    name: str,\n",
    "    datamodule: DataModule,\n",
    "):\n",
    "    best_model = model_cls.load_from_checkpoint(\n",
    "        checkpoint_path=f\"./data/checkpoints/{name}/model.ckpt\"\n",
    "    )\n",
    "    best_model.eval()\n",
    "\n",
    "    z = []\n",
    "\n",
    "    for batch in datamodule.all_dataloader():\n",
    "        z.append(best_model(batch))\n",
    "\n",
    "    return torch.cat(z, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfba585e-104c-4569-af91-c7cfbcf1fe85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1100d08f-11b1-4c37-8066-90ae5894f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_hparams = {\n",
    "    \"encoder_cls\": MultimodalEncoder,\n",
    "    \"modality_names\": [\"img_emb\", \"text_emb\"],\n",
    "    \"data_dims\": {\"img_emb\": 2048, \"text_emb\": 384}, \n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 30,\n",
    "    \"hidden_dims\": [256, 256, 256],\n",
    "    \"emb_dim\": 128,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 5e-4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c380503c-2551-4b53-b504-71e0ff02e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = DataModule(\n",
    "    train_path=\"data/cub/preprocessed_train.pkl\",\n",
    "    test_path=\"data/cub/preprocessed_test.pkl\",\n",
    "    batch_size=default_hparams[\"batch_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c770b70-2673-444d-85e1-935a6bc2632a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: ./data/logs/ImageTextAvgAE\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type              | Params\n",
      "----------------------------------------------\n",
      "0 | encoder | MultimodalEncoder | 952 K \n",
      "1 | decoder | MultimodalDecoder | 954 K \n",
      "2 | fusion  | AvgFusion         | 0     \n",
      "----------------------------------------------\n",
      "1.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 M     Total params\n",
      "7.625     Total estimated model params size (MB)\n",
      "/home/cicheck/.pyenv/versions/3.10.2/envs/representation/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/cicheck/.pyenv/versions/3.10.2/envs/representation/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91209ce9cd24f208d9ce70f94824391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cicheck/.pyenv/versions/3.10.2/envs/representation/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model_cls=MultimodalAE,\n",
    "    hparams={\n",
    "        \"name\": \"ImageTextAvgAE\",\n",
    "        \"fusion\": \"Avg\",\n",
    "        **default_hparams,\n",
    "    },\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab66c0-f6c6-45bb-b7da-0cd7580fd78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model_cls=MultimodalAE,\n",
    "    hparams={\n",
    "        \"name\": \"ImageTextMLPAE\",\n",
    "        \"fusion\": \"MLP\",\n",
    "        **default_hparams,\n",
    "    },\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde8646-73e5-44b9-8e2e-87a1cd2ff813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
