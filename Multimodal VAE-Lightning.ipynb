{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e335065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Type, Tuple\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from abc import abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4d07de",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02cf596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        super().__init__()\n",
    "        self._df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._df.shape[0]\n",
    "\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        index: int,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        return dict(self._df.iloc[index])\n",
    "\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        test_path: str,\n",
    "        batch_size: int = 64,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        train_df = pd.read_pickle(train_path)\n",
    "        test_df = pd.read_pickle(test_path)\n",
    "\n",
    "        self.df = {\n",
    "            \"train\": train_df,\n",
    "            \"test\": test_df,\n",
    "            \"all\": pd.concat([train_df, test_df]),\n",
    "        }\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(\"train\")\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(\"test\")\n",
    "\n",
    "    def all_dataloader(self) -> DataLoader:\n",
    "        return self._dataloader(\"all\")\n",
    "\n",
    "    def _dataloader(self, split: str) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            MyDataset(self.df[split]),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=split == \"train\",\n",
    "            num_workers=int(os.environ.get(\"NUM_WORKERS\", 0)),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d80d4e",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b7a5d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    \"\"\"https://arxiv.org/abs/1710.05941\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Parametrizes q(z|x).\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_dims: int,\n",
    "                 hidden_dims: int,\n",
    "                 out_dim: int,\n",
    "                 activation: Type[nn.Module]\n",
    "                 ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.out_dim = out_dim\n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Linear(in_dims, hidden_dims[0]),\n",
    "            self.activation(),\n",
    "            *[\n",
    "                layer\n",
    "                for idx in range(len(hidden_dims) - 1)\n",
    "                for layer in (nn.Linear(hidden_dims[idx], hidden_dims[idx + 1]), self.activation())\n",
    "            ],\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(hidden_dims[-1], out_dim * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_dim = self.out_dim\n",
    "        x = self.encode(x)\n",
    "        return x[:, :out_dim], x[:, out_dim:]\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_hparams(hparams, type_name):\n",
    "        return Encoder(\n",
    "            in_dims=hparams[\"data_dims\"][type_name],\n",
    "            hidden_dims=hparams[\"hidden_dims\"][type_name],\n",
    "            out_dim=hparams[\"emb_dim\"],\n",
    "            activation=hparams[\"activation\"],\n",
    "        )\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Parametrizes p(x|z).\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_dims: int,\n",
    "                 hidden_dims: int,\n",
    "                 out_dim: int,\n",
    "                 activation: Type[nn.Module]\n",
    "                 ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.out_dim = out_dim\n",
    "        self.activation = activation\n",
    "        self.decode = nn.Sequential(\n",
    "            nn.Linear(in_dims, hidden_dims[0]),\n",
    "            self.activation(),\n",
    "            *[\n",
    "                layer\n",
    "                for idx in range(len(hidden_dims) - 1)\n",
    "                for layer in (nn.Linear(hidden_dims[idx], hidden_dims[idx + 1]), self.activation())\n",
    "            ],\n",
    "            nn.Linear(hidden_dims[-1], out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # the input will be a vector of size |n_latents|\n",
    "        z = self.decode(z)\n",
    "        # returns reconstructed image/text embedding\n",
    "        return z  # NOTE: no sigmoid here. See in training\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_hparams(hparams, type_name):\n",
    "        return Decoder(\n",
    "            in_dims=hparams[\"emb_dim\"],\n",
    "            hidden_dims=hparams[\"hidden_dims\"][type_name][::-1],\n",
    "            out_dim=hparams[\"data_dims\"][type_name],\n",
    "            activation=hparams[\"activation\"],\n",
    "        )\n",
    "\n",
    "class ProductOfExperts(nn.Module):\n",
    "    \"\"\"Return parameters for product of independent experts.\n",
    "    See https://arxiv.org/pdf/1410.7827.pdf for equations.\n",
    "\n",
    "    @param mu: M x D for M experts\n",
    "    @param logvar: M x D for M experts\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, mu, logvar, eps=1e-8):\n",
    "        var = torch.exp(logvar) + eps\n",
    "        # precision of i-th Gaussian expert at point x\n",
    "        T = 1. / var\n",
    "        pd_mu = torch.sum(mu * T, dim=0) / torch.sum(T, dim=0)\n",
    "        pd_var = 1. / torch.sum(T, dim=0)\n",
    "        pd_logvar = torch.log(pd_var)\n",
    "        return pd_mu, pd_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20cda7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbo_loss(recon_img_emb, img_emb, recon_text_emb, text_emb, mu, logvar,\n",
    "              lambda_image=1.0, lambda_text=1.0, annealing_factor=1):\n",
    "    \n",
    "    image_bce, text_bce = 0, 0  # default params\n",
    "\n",
    "    if recon_img_emb is not None and img_emb is not None:\n",
    "        image_bce = F.mse_loss(recon_img_emb, img_emb) #torch.sum(binary_cross_entropy_with_logits(recon_img_emb, img_emb))\n",
    "\n",
    "    if recon_text_emb is not None and text_emb is not None:\n",
    "        text_bce = F.mse_loss(recon_text_emb, text_emb) #torch.sum(binary_cross_entropy_with_logits(recon_text_emb, text_emb))\n",
    "    \n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    KLD = torch.mean(\n",
    "        -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1),\n",
    "        dim=0,\n",
    "    )\n",
    "\n",
    "    ELBO = lambda_image * image_bce + lambda_text * text_bce + annealing_factor * KLD\n",
    "    return ELBO\n",
    "\n",
    "class BaseVAE(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hparams,\n",
    "                 image_encoder,\n",
    "                 image_decoder,\n",
    "                 text_encoder,\n",
    "                 text_decoder,):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(hparams)\n",
    "\n",
    "        self.image_encoder = image_encoder\n",
    "        self.image_decoder = image_decoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.text_decoder = text_decoder\n",
    "\n",
    "    def training_step(self, batch, batch_idx: int):\n",
    "        return {\"loss\": self._common_step(batch)}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = self._summarize_outputs(outputs)\n",
    "\n",
    "        self.log(\"step\", self.trainer.current_epoch)\n",
    "        self.log(\"train/loss\", avg_loss, on_epoch=True, on_step=False)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx: int):\n",
    "        return {\"loss\": self._common_step(batch)}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = self._summarize_outputs(outputs)\n",
    "\n",
    "        self.log(\"step\", self.trainer.current_epoch)\n",
    "        self.log(\"val/loss\", avg_loss, on_epoch=True, on_step=False)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _common_step(self, batch) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def _summarize_outputs(outputs):\n",
    "        losses = [out[\"loss\"] for out in outputs]\n",
    "        avg_loss = np.mean([loss.cpu() for loss in losses])\n",
    "        return avg_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            params=self.parameters(),\n",
    "            lr=self.hparams[\"lr\"],\n",
    "            weight_decay=self.hparams[\"weight_decay\"],\n",
    "        )\n",
    "\n",
    "class MVAE(BaseVAE):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__(\n",
    "            hparams=hparams,\n",
    "            image_encoder=Encoder.from_hparams(hparams, 'image'),\n",
    "            image_decoder=Decoder.from_hparams(hparams, 'image'),\n",
    "            text_encoder=Encoder.from_hparams(hparams, 'text'),\n",
    "            text_decoder=Decoder.from_hparams(hparams, 'text')\n",
    "        )\n",
    "        self.product_of_experts = ProductOfExperts()\n",
    "        self.n_latents = hparams[\"emb_dim\"]\n",
    "        self.lambda_image=hparams[\"lambda_image\"]\n",
    "        self.lambda_text=hparams[\"lambda_text\"]\n",
    "        self.annealing_factor=hparams[\"annealing_factor\"]\n",
    "\n",
    "    def get_representation(self, img_emb, text_emb):\n",
    "        mu, logvar = self.forward_encoder(img_emb, text_emb)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return z\n",
    "        \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = std.data.new(std.size()).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:  # return mean during inference\n",
    "            return mu\n",
    "\n",
    "    def forward(self, img_emb=None, text_emb=None):\n",
    "        mu, logvar = self.forward_encoder(img_emb, text_emb)\n",
    "        # reparametrization trick to sample\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        # reconstruct inputs based on that gaussian\n",
    "        image_recon, text_recon = self.forward_decoder(z)\n",
    "        return image_recon, text_recon, mu, logvar\n",
    "\n",
    "    def forward_encoder(self, img_emb=None, text_emb=None):\n",
    "        if img_emb is not None:\n",
    "            batch_size = img_emb.size(0)\n",
    "        else:\n",
    "            batch_size = text_emb.size(0)\n",
    "\n",
    "        use_cuda = next(self.parameters()).is_cuda  # check if CUDA\n",
    "        mu, logvar = prior_expert((1, batch_size, self.n_latents),\n",
    "                                  use_cuda=use_cuda)\n",
    "        if img_emb is not None:\n",
    "            image_mu, image_logvar = self.image_encoder(img_emb)\n",
    "            mu = torch.cat((mu, image_mu.unsqueeze(0)), dim=0)\n",
    "            logvar = torch.cat((logvar, image_logvar.unsqueeze(0)), dim=0)\n",
    "\n",
    "        if text_emb is not None:\n",
    "            text_mu, text_logvar = self.text_encoder(text_emb)\n",
    "            mu = torch.cat((mu, text_mu.unsqueeze(0)), dim=0)\n",
    "            logvar = torch.cat((logvar, text_logvar.unsqueeze(0)), dim=0)\n",
    "\n",
    "        # product of experts to combine gaussians\n",
    "        mu, logvar = self.product_of_experts(mu, logvar)\n",
    "        return mu, logvar\n",
    "\n",
    "    def forward_decoder(self, z):\n",
    "        image_recon = self.image_decoder(z)\n",
    "        text_recon = self.text_decoder(z)\n",
    "        return image_recon, text_recon\n",
    "    \n",
    "    def _common_step(self, batch) -> torch.Tensor:\n",
    "        img_emb = batch['img_emb']\n",
    "        text_emb = batch['text_emb']\n",
    "        train_loss = 0  # accumulate train loss here so we don't store a lot of things.\n",
    "\n",
    "        # compute ELBO using all data (``complete\")\n",
    "        recon_img_emb, recon_text_emb, mu, logvar = self.forward(img_emb, text_emb)\n",
    "        train_loss += elbo_loss(recon_img_emb, img_emb, recon_text_emb, text_emb,\n",
    "                                mu=mu,\n",
    "                                logvar=logvar,\n",
    "                                lambda_image=self.lambda_image,\n",
    "                                lambda_text=self.lambda_text,\n",
    "                                annealing_factor=self.annealing_factor\n",
    "                                )\n",
    "\n",
    "        # compute ELBO using only img_emb data\n",
    "        recon_img_emb, _, mu, logvar = self.forward(img_emb=img_emb)\n",
    "        train_loss += elbo_loss(recon_img_emb, img_emb, None, None,\n",
    "                                mu=mu,\n",
    "                                logvar=logvar,\n",
    "                                lambda_image=self.lambda_image,\n",
    "                                lambda_text=self.lambda_text,\n",
    "                                annealing_factor=self.annealing_factor\n",
    "                                )\n",
    "\n",
    "        # compute ELBO using only text data\n",
    "        _, recon_text_emb, mu, logvar = self.forward(text_emb=text_emb)\n",
    "        train_loss += elbo_loss(None, None, recon_text_emb, text_emb,\n",
    "                                mu=mu,\n",
    "                                logvar=logvar,\n",
    "                                lambda_image=self.lambda_image,\n",
    "                                lambda_text=self.lambda_text,\n",
    "                                annealing_factor=self.annealing_factor\n",
    "                                )\n",
    "        return train_loss\n",
    "    \n",
    "    \n",
    "\n",
    "def prior_expert(size, use_cuda=False):\n",
    "    \"\"\"Universal prior expert. Here we use a spherical\n",
    "    Gaussian: N(0, 1).\n",
    "\n",
    "    @param size: integer\n",
    "                 dimensionality of Gaussian\n",
    "    @param use_cuda: boolean [default: False]\n",
    "                     cast CUDA on variables\n",
    "    \"\"\"\n",
    "    mu = torch.zeros(size)\n",
    "    logvar = torch.log(torch.ones(size))\n",
    "    if use_cuda:\n",
    "        mu, logvar = mu.cuda(), logvar.cuda()\n",
    "    return mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7eabdc",
   "metadata": {},
   "source": [
    "# Train/Test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7f78d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model_cls: Type[pl.LightningModule],\n",
    "    hparams,\n",
    "    datamodule: DataModule,\n",
    "    accelerator=\"gpu\",\n",
    "):\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    model = model_cls(hparams)\n",
    "\n",
    "    model_chkpt = ModelCheckpoint(\n",
    "        dirpath=f\"./data/checkpoints/{hparams['name']}/\",\n",
    "        filename=\"model\",\n",
    "        monitor=\"val/loss\",\n",
    "        mode=\"min\",\n",
    "        verbose=True,\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        logger=TensorBoardLogger(\n",
    "            save_dir=\"./data/logs\",\n",
    "            name=hparams[\"name\"],\n",
    "            default_hp_metric=False,\n",
    "        ),\n",
    "        callbacks=[model_chkpt],\n",
    "        num_sanity_val_steps=0,\n",
    "        log_every_n_steps=1,\n",
    "        max_epochs=hparams[\"num_epochs\"],\n",
    "        accelerator=accelerator,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "659beb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_hparams = {\n",
    "    \"data_dims\": {\"image\": 2048, \"text\": 384}, \n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 3,\n",
    "    \"hidden_dims\": {\n",
    "        \"image\": [1024, 512, 256],#lst[::-1]\n",
    "        \"text\": [256, 256, 256],\n",
    "    },\n",
    "    \"emb_dim\": 128,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 5e-4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa26c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = DataModule(\n",
    "    train_path=\"data/cub/preprocessed_train.pkl\",\n",
    "    test_path=\"data/cub/preprocessed_test.pkl\",\n",
    "    batch_size=default_hparams[\"batch_size\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e22ad30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/erthax/Programming/Uczenie Reprezentacji/venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory /home/erthax/Programming/Uczenie Reprezentacji/reprezentacja-projekt/data/checkpoints/ImageTextVAE exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type             | Params\n",
      "--------------------------------------------------------\n",
      "0 | image_encoder      | Encoder          | 2.8 M \n",
      "1 | image_decoder      | Decoder          | 2.8 M \n",
      "2 | text_encoder       | Encoder          | 295 K \n",
      "3 | text_decoder       | Decoder          | 263 K \n",
      "4 | product_of_experts | ProductOfExperts | 0     \n",
      "--------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.674    Total estimated model params size (MB)\n",
      "/home/erthax/Programming/Uczenie Reprezentacji/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/erthax/Programming/Uczenie Reprezentacji/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225b1925f38e4c00a26b0b449b409383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 0: 'val/loss' reached 0.00186 (best 0.00186), saving model to '/home/erthax/Programming/Uczenie Reprezentacji/reprezentacja-projekt/data/checkpoints/ImageTextVAE/model-v6.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1: 'val/loss' reached 0.00186 (best 0.00186), saving model to '/home/erthax/Programming/Uczenie Reprezentacji/reprezentacja-projekt/data/checkpoints/ImageTextVAE/model-v6.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 2: 'val/loss' reached 0.00185 (best 0.00185), saving model to '/home/erthax/Programming/Uczenie Reprezentacji/reprezentacja-projekt/data/checkpoints/ImageTextVAE/model-v6.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model_cls=MVAE,\n",
    "    hparams={\n",
    "        \"name\": \"ImageTextVAE\",\n",
    "        \"activation\" : Swish,\n",
    "        \"lambda_image\" : 1.0,\n",
    "        \"lambda_text\" : 1.0,\n",
    "        \"annealing_factor\" : 1.0,\n",
    "        **default_hparams,\n",
    "    },\n",
    "    datamodule=datamodule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9df21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "24b25394",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_embeddings(\n",
    "    datamodule,\n",
    "):\n",
    "    best_model = load_checkpoint(\"./trained_models/model_best.pth.tar\", use_cuda=True)\n",
    "    best_model.cuda()\n",
    "    best_model.eval()\n",
    "\n",
    "    z = []\n",
    "    for batch in datamodule.train_dataloader():\n",
    "        text_emb = batch[\"text_emb\"].cuda()\n",
    "        img_emb = batch[\"img_emb\"].cuda()\n",
    "        z.append(best_model.get_representation(img_emb, text_emb))\n",
    "    return torch.cat(z, dim=0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b082e57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
