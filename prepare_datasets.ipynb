{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4652a5-8aa6-4521-a900-66bb238d45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Type\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms as T\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import normalize as normalize_emb\n",
    "from torchvision import models as tv_models\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2a345ac-016f-4eb1-baa5-a095e377b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc1e33f7-0af8-4f1b-99b2-af8b5dc6aa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    # Transformations\n",
    "    resize = T.Resize((224, 224))\n",
    "    normalize = T.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "    pil_to_image = T.PILToTensor()\n",
    "    # Get models used to preprocess features\n",
    "    mini_lm = SentenceTransformer(\"all-MiniLM-L6-v2\").to(DEVICE)\n",
    "    resnet = tv_models.resnet50(pretrained=True).to(DEVICE)\n",
    "    resnet.eval()\n",
    "    resnet.fc = nn.Identity()\n",
    "    # Preprocess\n",
    "    preprocessed = []\n",
    "    for sample in tqdm(\n",
    "        iterable=dataset,\n",
    "        total=len(dataset),\n",
    "        desc=\"Processing data\",\n",
    "    ):\n",
    "        image = sample[\"image\"]\n",
    "        label = sample[\"label\"]\n",
    "        # There are 4 images in \"L\" format\n",
    "        if sample[\"image\"].mode == \"L\":\n",
    "            continue\n",
    "        image = pil_to_image(image).float().to(DEVICE)\n",
    "        resized_img = resize(image)\n",
    "        normalized_img = normalize(resized_img)\n",
    "        for description in sample[\"description\"].split(\"\\n\"):\n",
    "            if not description:\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                # Added batch dim\n",
    "                img_emb = resnet(normalized_img.unsqueeze(dim=0))\n",
    "                text_emb = mini_lm.encode(\n",
    "                    sentences=description,\n",
    "                    convert_to_tensor=True,\n",
    "                )\n",
    "            preprocessed.append(\n",
    "                {\n",
    "                    \"img_emb\": normalize_emb(img_emb[0], dim=0), # Drop batch dim\n",
    "                    \"text_emb\": normalize_emb(text_emb, dim=0),\n",
    "                    \"image_index\": sample[\"img_index\"],\n",
    "                    \"text\": description,\n",
    "                    \"label\": sample[\"label\"],\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b81b4-c67f-41f8-9c95-2b79e0eeb776",
   "metadata": {},
   "source": [
    "## Preprocess Cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12147714-c90d-4446-8a2b-d6b9e194f8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data/cub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaf8efd9-c2e2-42b2-8d98-5b7fac250f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cc6204-hackaton-cub-dataset (/home/cicheck/.cache/huggingface/datasets/alkzar90___cc6204-hackaton-cub-dataset/default/0.0.0/de850c9086bff0dd6d6eab90f79346241178f65e1a016a50eec240ae9cdf2064)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee8c34b84926421cb11bf22c31105bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"alkzar90/CC6204-Hackaton-Cub-Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcd1cc3e-7db5-4fc1-8c9d-c645918f970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orginal type is not mutable\n",
    "dataset = {\n",
    "    \"train\": list(dataset[\"train\"]),\n",
    "    \"test\": list(dataset[\"test\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b777be22-c265-471e-8f70-ab6fdace3171",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\", \"test\"]:\n",
    "    for index, sample in enumerate(dataset[split]):\n",
    "        sample[\"img_index\"] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c1278c0-0987-455f-a043-0f53d42241e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████████████████| 5994/5994 [11:03<00:00,  9.03it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train = preprocess_data(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4389c0e-de73-4fed-b7a9-490898ef925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train.to_pickle(\"data/cub/preprocessed_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8862ca7a-130b-452c-aff6-927e4fd44979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cicheck/.pyenv/versions/3.10.2/envs/representation/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/cicheck/.pyenv/versions/3.10.2/envs/representation/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Processing data: 100%|██████████████████████| 5794/5794 [11:01<00:00,  8.77it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_test = preprocess_data(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e25463b-47ab-4977-96ae-a0cdac3ee659",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_test.to_pickle(\"data/cub/preprocessed_test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ec3ea-a0df-4d3d-8cf6-e036ac8ac9e5",
   "metadata": {},
   "source": [
    "## Preprocess Hatefull Meme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62218218-3267-4ef3-afb8-28b4ebb407ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data/meme/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "623bd07d-2dcf-413d-b244-d51ddafc9460",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(path_or_buf=\"data/heatfull_meme/data/train.jsonl\", lines=True)\n",
    "test = pd.read_json(path_or_buf=\"data/heatfull_meme/data/test.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "cc6e1df5-6828-4c7b-b5f2-edce45d30414",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.to_dict(\"records\")\n",
    "test = test.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "29e2cd8c-2064-4bda-b030-5b72d5c95205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 42953,\n",
       " 'img': 'img/42953.png',\n",
       " 'label': 0,\n",
       " 'text': 'its their character not their color that matters'}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "dc73496e-b6c7-47cf-a32d-73e65de9d434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.JpegImagePlugin.JpegImageFile"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"train\"][0][\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "74856c21-0c01-49ed-8125-d14cf082a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5ed9a8ab-6fd5-47b9-b59e-f3e37159a398",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 24] Too many open files: 'data/heatfull_meme/data/<PIL.PngImagePlugin.PngImageFile image mode=RGB size=265x400 at 0x7F53405B56C0>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [183], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m train:\n\u001b[0;32m----> 2\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/heatfull_meme/data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m img\n\u001b[1;32m      4\u001b[0m     img\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/representation/lib/python3.10/site-packages/PIL/Image.py:3092\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3089\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3092\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3093\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3095\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 24] Too many open files: 'data/heatfull_meme/data/<PIL.PngImagePlugin.PngImageFile image mode=RGB size=265x400 at 0x7F53405B56C0>'"
     ]
    }
   ],
   "source": [
    "for sample in train:\n",
    "    img = Image.open(f\"data/heatfull_meme/data/{sample['img']}\")\n",
    "    sample[\"img\"] = img\n",
    "    img.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215104f7-35fa-4e6e-a0b0-d6472a8fa952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
